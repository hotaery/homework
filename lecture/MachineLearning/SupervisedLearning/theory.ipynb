{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概率论前置知识\n",
    "## 集合\n",
    "集合是具有某种特性事物的整体，或者一些对象的汇集，构成集合的事物或者对象称为元素。\n",
    "\n",
    "比如所有哺乳动物就是一个集合，或者与原点距离为r的点的集合，因此集合不仅可以使用枚举量来表，也可以使用代数表示：\n",
    "$$\\{(x,y)|x^2+y^2=r^2\\}$$\n",
    "\n",
    "$$\\{星期一，星期二，星期三，星期四，星期五\\}$$\n",
    "\n",
    "集合满足三个特性：\n",
    "- 无序性，集合中的每个元素地位都是相同的，元素之间是无序的\n",
    "- 互异性，集合中没有两个元素是相同的\n",
    "- 确定性，给定一个集合，人给一个元素只有属于和不属于该集合\n",
    "\n",
    "## 样本空间\n",
    "包含所有结果的集合$\\Omega$称为样本空间，样本空间的元素称为基本结果或者样本点。比如投掷一枚硬币，其样本空间为：\n",
    "$$\\Omega=\\{正面，反面\\}$$\n",
    "当然样本空间也可以包含无数的样本点，比如人类的身高:\n",
    "$$\\Omega=\\{x|x \\in \\mathcal{R}\\}$$\n",
    "\n",
    "## 事件\n",
    "事件$\\mathcal{A}$是样本空间的一个子集：\n",
    "$$\\mathcal{A}\\subseteq\\Omega$$\n",
    "比如扔一次骰子得到的点数的样本空间为：\n",
    "$$\\Omega=\\{1,2,3,4,5,6\\}$$\n",
    "那么事件$\\mathcal{A}$是得到的点数为偶数，该事件为：\n",
    "$$\\mathcal{A}=\\{2,4,6\\}$$\n",
    "\n",
    "互斥事件：$P(A \\cap B) = 0$\n",
    "\n",
    "独立事件：$P(AB)=P(A)P(B) => P(A|B)=P(A), P(B|A)=P(B)$\n",
    "\n",
    "独立和互斥没有关系，独立事件也可能会相交。\n",
    "\n",
    "## 概率\n",
    "对于某个样本空间$\\Omega$，对于其中任一事件$A$，定义函数$P$，若满足以下三大公理：\n",
    "- 非负性公理\n",
    "$$P(A) \\ge 0$$\n",
    "- 规范顶公理\n",
    "$$P(\\Omega)=1$$\n",
    "- 可加性公理\n",
    "\n",
    "    设$A_1、A_2、\\cdots$为两两不相容事件，即$A_i\\cap A_j=\\varnothing（i\\ne j）$，那么\n",
    "\n",
    "    $$P(A_1\\cup A_2\\cup\\cdots) = P(A_1)+P(A_2)+\\cdots$$\n",
    "\n",
    "称$P(A)$为概率函数，$P(A)$称为事件$A$的概率。\n",
    "\n",
    "## 条件概率\n",
    "设$A$和$B$是样本空间$\\Omega$中的两个事件，并且有$P(B) \\gt 0$，那么有\n",
    "\n",
    "$$P(A|B)=\\frac{P(A\\cap B)}{P(B)}$$\n",
    "\n",
    "这称为假设条件$B$下的事件$A$的概率\n",
    "\n",
    "条件概率可以理解为将样本空间$\\Omega$缩小到事件$B$的大小，因此条件概率是局部概率。\n",
    "\n",
    "## 全概率\n",
    "设$A_1, A_2, \\cdots A_n$满足\n",
    "\n",
    "$$A_i\\cap A_j=\\varnothing , (i\\ne j)\\quad 且\\quad P(\\bigcup_{i=1}^{n}A_i)=1$$\n",
    "\n",
    "若$P(A_i) > 0,i=1,2,\\cdots,n$，那么对于任意事件$B$：\n",
    "\n",
    "$$P(B)=\\sum_{i=1}^{n}P(A_i)P(B|A_i)$$\n",
    "\n",
    "可以通过条件概率推导而来\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    P(B) &= \\sum_{i=1}^{n}P(A_iB) \\\\\n",
    "    &= \\sum_{i=1}^{n}P(A_i)P(B|A_i)\n",
    "\\end{aligned}$$\n",
    "\n",
    "## 贝叶斯定理\n",
    "对于同一样本空间$\\Omega$中的随机事件$A、B$，若$P(B) \\ne 0$，有：\n",
    "$$P(A|B)=\\frac{P(A)}{P(B)}P(B|A)$$\n",
    "\n",
    "可以通过条件概率推导而来\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    P(B|A)\\cdot \\frac{P(A)}{P(B)}\n",
    "        &=\\frac{P(AB)}{P(A)}\\cdot \\frac{P(A)}{P(B)}\\\\\n",
    "        \\\\\n",
    "        &=\\frac{P(AB)}{P(B)}\\\\\n",
    "        \\\\\n",
    "        &=P(A|B)\n",
    "\\end{aligned}$$\n",
    "\n",
    "## 随机变量\n",
    "如果把事件$\\omega$映射成一个实值，那么可以在样本空间下定义一个函数\n",
    "$$X=X(\\omega),\\quad \\omega\\in\\Omega$$\n",
    "将$X$称为随机变量。\n",
    "\n",
    "随机变量分为两种\n",
    "- 离散型随机变量\n",
    "- 连续型随机变量\n",
    "\n",
    "## 概率质量函数和分布\n",
    "概率质量函数(Probability Mass Funcion, PMF)是随机变量发生的概率。假设对于离散的随机变量，其全部可能的是为$x_1,x_2,...$，那么\n",
    "$$p(x_i)=P(X=x_i), i=1,2...$$\n",
    "也成为随机变量$X$的概率分布，表示为\n",
    "$$X\\sim p(x)$$\n",
    "读作$X$服从$p(x)$的概率分布\n",
    "\n",
    "概率质量函数$p(x), x\\in \\{x_i\\},i=1,2,\\cdots$需要满足：\n",
    "- 非负性\n",
    "$$p(x_i) \\ge 0$$\n",
    "- 规范性\n",
    "$$\\sum_{i=1}^{\\infty}p(x_i)=1$$\n",
    "\n",
    "## 期望\n",
    "对于离散的随机变量$X$，如果有\n",
    "\n",
    "$$\\sum_{i=1}^{\\infty}|x_i|p(x_i) < \\infty$$\n",
    "\n",
    "那么\n",
    "\n",
    "$$E(X)=\\sum_{i=1}^{\\infty}x_ip(x_i)$$\n",
    "\n",
    "为随机变量$X$的期望(expectation)或者均值(mean)。\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    p(x) &\\ge 0 \\\\\n",
    "    \\sum_i p(x_i) &= 1\n",
    "\\end{aligned}$$\n",
    "\n",
    "## 方差与标准差\n",
    "方差\n",
    "\n",
    "$$Var(X)=E\\left[\\Big(X-E(X)\\Big)^2\\right]$$\n",
    "\n",
    "记作$\\sigma^2$或者$\\sigma_X^2$。\n",
    "\n",
    "由于方差的单位是随机变量单位的平方，对其开根号就是标准差，记作$\\sigma$或者$\\sigma_X$。\n",
    "\n",
    "$$\\sigma(X)=\\sqrt{Var(X)}$$\n",
    "\n",
    "方差和标准差用于描述随机变量的离散程度，值越大，说明随机变量分布的越离散。\n",
    "\n",
    "## 伯努利分布\n",
    "样本空间中只包含两个元素\n",
    "\n",
    "$$ \\Omega={\\omega_1, \\omega_2}$$\n",
    "\n",
    "若$0\\le p \\le 1$时有\n",
    "\n",
    "$$P(X=x)=p(x)=\n",
    "\\begin{cases}\n",
    "p,&x=1\\\\\n",
    "1-p,&x=0 \n",
    "\\end{cases}$$\n",
    "\n",
    "那么称为伯努利分布，或者0-1分布。\n",
    "\n",
    "## 二项分布\n",
    "对于$n$次伯努利实验，如果每次得到1的概率为$p$，设随机变量:\n",
    "\n",
    "$$ X=\\text{得到1的次数}$$\n",
    "\n",
    "那么有\n",
    "\n",
    "$$p(k)=P(X=k)={n\\choose k}p^k(1-p)^{n-k},\\quad k=0,1,\\cdots,n$$\n",
    "\n",
    "称为随机变量$X$的二项分布，记作\n",
    "\n",
    "$$X\\sim b(n,p)$$\n",
    "\n",
    "## 正态分布\n",
    "\n",
    "$$p(x)=\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}},\\quad -\\infty < x < +\\infty$$\n",
    "\n",
    "正态分布也称为高斯分布，记作\n",
    "\n",
    "$$X\\sim N(\\mu,\\sigma^2)$$\n",
    "\n",
    "## 泊松分布\n",
    "\n",
    "$$P(X=k)=\\frac{\\lambda^k}{k!}e^{-\\lambda},\\quad k=0,1,2,\\cdots$$\n",
    "\n",
    "其中泊松分布的数学期望和方差为\n",
    "\n",
    "$$E(X)=\\lambda,\\quad Var(X)=\\lambda$$\n",
    "\n",
    "## 多维随机变量\n",
    "如果$X_1(\\omega), X_2(\\omega), ..., X_n(\\omega)$是定义在同一个样本空间下$\\Omega={\\omega}$下的n个随机变量，则称\n",
    "$$X(\\omega)=(X_1(\\omega),X_2(\\omega),\\cdots,X_n(\\omega))$$\n",
    "为n维随机变量或者随机向量。\n",
    "\n",
    "## 联合分布\n",
    "如果二维随机变量$(X, Y)$所有可能的取值为$(x_i, y_i),i,j=1,2,...$，并且两个随机变量同时发生的概率为：\n",
    "$$p_{ij}=P(X=x_i,Y=y_j)=P(X=x_i\\ 且\\ Y=y_j),\\quad i,j=1,2,\\cdots$$\n",
    "并且满足：\n",
    "- 非负性\n",
    "- 规范性和可加性，也就是累加和为1\n",
    "\n",
    "那么$P(X, Y)$就是联合概率密度函数(Joint Probability Mass Function)，也成为联合分布。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习的可行性\n",
    "\n",
    "## 重要的名词和概念\n",
    "### 上帝函数\n",
    "假设存在一个全能的函数$f$，不论输入什么特征向量，其总能给出正确的输出，那么该函数$f$就称为上帝函数。\n",
    "$$y_i=f(\\boldsymbol{x_i}),\\quad \\boldsymbol{x_i}\\in\\mathcal{X},y_i\\in\\mathcal{Y}$$\n",
    "也可以写成\n",
    "$$f:\\mathcal{X}\\to\\mathcal{Y}$$\n",
    "集合$\\mathcal{X}$包含上帝函数所有的输入，称为输入空间(input space)，集合$\\mathcal{Y}$称为输出空间(output space)。\n",
    "\n",
    "### 假设空间\n",
    "假设空间(hypothesis space)是所有可能的有输入空间到输出空间映射的集合，对于感知机来说，假设空间就是所有可能的超平面：\n",
    "$$\\mathcal{H}=\\{\\ h(\\boldsymbol{x})=\\operatorname{sign}(\\boldsymbol{w}\\cdot\\boldsymbol{x}+b)\\ \\}$$\n",
    "\n",
    "### 学习算法\n",
    "学习算法(learning algorithm)是从假设空间中挑选合适的映射的算法，对于感知机来说，有暴力算法、口袋算法等。\n",
    "\n",
    "学习模型$\\mathcal{M}$就是假设空间$\\mathcal{H}$+学习算法。\n",
    "\n",
    "### 泛化误差\n",
    "对于学习模型找到的函数$h$，需要评估其泛化能力，机器学习中使用泛化误差来评估泛化能力：\n",
    "$$R(h)=E_{(\\boldsymbol{x},y)\\sim\\mathcal{D}}[I(h(\\boldsymbol{x})\\ne y)]$$\n",
    "\n",
    "其中$I$为示性函数\n",
    "$$I(\\omega)=\n",
    "\\begin{cases}\n",
    "    1,& 事件\\ \\omega\\ 发生\\\\\n",
    "    0,& 事件\\ \\omega\\ 未发生\n",
    "\\end{cases}$$\n",
    "$\\mathcal{D}$是有上帝函数$f$产生的分布\n",
    "\n",
    "因此泛化误差$R(h)$的最大值为1，这是因为分布$\\mathcal{D}$的概率质量函数累加和为1，最大值的情况是$h$对于分布$\\mathcal{D}$的预测全是错误的，那么泛化误差就是概率质量函数累加和。\n",
    "\n",
    "### 经验误差\n",
    "泛化误差是在分布$\\mathcal{D}$上计算得到的，由于上帝函数$f$是未知的，因此分布$\\mathcal{D}$也是未知的，因此无法计算出来$R(h)$。为了评估函数$h$的泛化能力，我们引入经验误差，经验误差是在数据集$D$上计算得到的\n",
    "$$\\hat{R}_D(h)=\\frac{1}{n}\\sum_{i=1}^{n}I(h(\\boldsymbol{x_i})\\ne y_i)$$\n",
    "其中$I$是示性函数，$n$是数据集$D$中元素个数。\n",
    "\n",
    "这里需要注意，经验误差$\\hat{R}_D(h)$是在数据集$D$上计算得到的，可能和泛化误差$R(h)$是不相等的，但是我们假设数据集$D$是分布$\\mathcal{D}$的独立同分布，这样经验误差$\\hat{R}_D(h)$是泛化误差$R(h)$的无偏估计。用形象一点的语言来说，假如泛化误差是靶心，那么不同的数据集$D$计算出来的经验误差$\\hat{R}_D(h)$会在靶心附近变化，因此可以把经验误差$\\hat{R}_D(h)$当作泛化误差$R(h)$的近似。\n",
    "\n",
    "### 大概率近似正确(PAC)\n",
    "根据霍夫丁不等式：\n",
    "$$P\\left(\\left|\\hat{R}_D(h)-R(h)\\right| > \\epsilon \\right) \\le 2e^{-2\\epsilon^2 N}$$\n",
    "\n",
    "其中$N$是数据集的大小，这个不等式表明经验误差$\\hat{R}_D(h)$和泛化误差$R(h)$差值为$\\epsilon$的概率。\n",
    "\n",
    "当数据集$D$越大，那么$\\hat{R}_D(h)$和$R(h)$会大概率非常接近，使用机器学习的术语就是\n",
    "$$“\\hat{R}_D(h) = R(h)”\\ \\text{是 PAC 的}$$\n",
    "其中PAC(probability approximately correct)\n",
    "\n",
    "K-折交叉验证式在验证集上计算$\\hat{R}_D(h)$的，K折后计算一个平均值作为这个epoch的$\\hat{R}_D(h)$，相当于使用更大的数据集$D$来计算$\\hat{R}_D(h)$，因此能够更准确的反应泛化误差$R(h)$\n",
    "\n",
    "### 经验误差最小原则\n",
    "机器学习算法的设计原则就是，利用独立同分布的数据集$\\mathcal{D}$，用各种方法尽量去遍历假设空间$\\mathcal{H}$，从中选择使得$\\hat{R}_D(h)$最小的$h$作为最终的$g$，这就是经验误差最小原则(empirical rise minimization, ERM)\n",
    "$$g=\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\hat{R}_D(h)$$\n",
    "\n",
    "对于大小为$M$的假设空间$\\mathcal{H}$，怎么确定找到的$g$是PAC的。令事件$A_i$是假设空间中第$i$元素是PAC的，即：\n",
    "$$A_i=(\\left|\\hat{R}_D(h_i)-R(h_i)\\right| > \\epsilon)$$\n",
    "那么有\n",
    "$$P(A_i)=P\\left(\\left|\\hat{R}_D(h_i)-R(h_i)\\right| > \\epsilon \\right) \\le 2e^{-2\\epsilon^2 N}$$\n",
    "假设$\\mathcal{H}$中所有元素都是PAC的，那么找到的$g$肯定也是PAC的，那么有\n",
    "$$P\\left(\\left|\\hat{R}_D(g)-R(g)\\right| > \\epsilon \\right)\\le P\\left(\\bigcup_{i=1}^{M}A_i\\right) \\le 2Me^{-2\\epsilon^2 N}$$\n",
    "\n",
    "因此只要数据集$D$足够大或者假设空间$\\mathcal{H}$是有限的，可以保证最终找的$g$是PAC的，也就是机器学习是可行的。\n",
    "\n",
    "### 成长函数、断点和VC维\n",
    "成长函数是描述假设空间$\\mathcal{H}$大小和数据集$D$大小的关系：\n",
    "$$\\text{Growth Function}=m_{\\mathcal{H}}(N)$$\n",
    "\n",
    "断点(break point)可以理解为函数断点，表示在某个$N$下，成长函数和$N-1$之前发生了突变，记为$k$，并且成长函数满足\n",
    "$$m_{\\mathcal{H}}(N)\\le N^{k-1},\\quad N\\ge 2, k\\ge 3$$\n",
    "\n",
    "那么有\n",
    "$$P\\left(\\left|\\hat{R}_D(g)-R(g)\\right| > \\epsilon \\right)\\le 4\\cdot (2N)^{k-1}\\cdot e^{-\\frac{1}{8}\\epsilon^2 N},N\\ 足够大$$\n",
    "固定$\\epsilon$和$k$后，上述是一个先递增后递减的函数，那么表明当数据集$D$足够大，保证最终找到$g$是PAC的。\n",
    "\n",
    "断点越大，说明模型越复杂，比如在二分类下，数据集$D$中所有可能的情况为$N^2$，如果断点为$k$，那么表示模型在数据集$D$大小为$k$时无法处理所有情况，即$k^2$，因此如果$k$越大，说明模型在数据集$D$大小小于$k$能够处理所有情况，因此$k$越大，说明模型能够处理的可能性越多，也就是模型越复杂。在机器学习中我们使用VC维(VC dimension)来描述模型的复杂度：\n",
    "$$d_{vc}=k-1$$\n",
    "\n",
    "### 模型的偏差和方差\n",
    "模型是\n",
    "$$模型\\mathcal{M} = 假设空间\\mathcal{H} + 算法\\mathcal{A}$$\n",
    "模型的偏差是模型的固有属性，是上帝函数$f$和假设空间$\\mathcal{H}$的距离，如果需要减小偏差，可以通过换一个模型也就是换一个假设空间来尝试。\n",
    "\n",
    "模型的方差是假设空间中误差最小的$h^*$实际学习到的$g$之间的距离，因此如果选择的模型较为复杂，也就是$d_{vc}$比较大，如果数据集$D$不够大，可能找到的$g$和$h^*$距离较远，也就是方差比较大。\n",
    "\n",
    "数据集D有限的情况下，机器学习的主要矛盾是，偏差减小方差会增大，偏差增加方差减小。用动画来表示就是，当假设空间远离f时，就会缩小；反之，则会增大。\n",
    "\n",
    "### 欠拟合和过拟合\n",
    "- 欠拟合表示在训练集和验证集上的误差都比较大，解决方案是尝试其他模型来减少偏差\n",
    "- 过拟合表示在训练集上误差比较小，在验证集上误差比较大，解决方案是减少方差，比如尝试简单点的模型"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
