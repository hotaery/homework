{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 概率论前置知识\n",
    "## 集合\n",
    "集合是具有某种特性事物的整体，或者一些对象的汇集，构成集合的事物或者对象称为元素。\n",
    "\n",
    "比如所有哺乳动物就是一个集合，或者与原点距离为r的点的集合，因此集合不仅可以使用枚举量来表，也可以使用代数表示：\n",
    "$$\\{(x,y)|x^2+y^2=r^2\\}$$\n",
    "\n",
    "$$\\{星期一，星期二，星期三，星期四，星期五\\}$$\n",
    "\n",
    "## 样本空间\n",
    "包含所有结果的集合$\\Omega$称为样本空间，样本空间的元素称为基本结果或者样本点。比如投掷一枚硬币，其样本空间为：\n",
    "$$\\Omega=\\{正面，反面\\}$$\n",
    "当然样本空间也可以包含无数的样本点，比如人类的身高:\n",
    "$$\\Omega=\\{x|x \\in \\mathcal{R}\\}$$\n",
    "\n",
    "## 事件\n",
    "事件$\\mathcal{A}$是样本空间的一个子集：\n",
    "$$\\mathcal{A}\\subseteq\\Omega$$\n",
    "比如扔一次骰子得到的点数的样本空间为：\n",
    "$$\\Omega=\\{1,2,3,4,5,6\\}$$\n",
    "那么事件$\\mathcal{A}$是得到的点数为偶数，该事件为：\n",
    "$$\\mathcal{A}=\\{2,4,6\\}$$\n",
    "\n",
    "## 随机变量\n",
    "如果把事件$\\omega$映射成一个实值，那么可以在样本空间下定义一个函数\n",
    "$$X=X(\\omega),\\quad \\omega\\in\\Omega$$\n",
    "将$X$称为随机变量。\n",
    "\n",
    "## 概率质量函数和分布\n",
    "概率质量函数(Probability Mass Funcion, PMF)是随机变量发生的概率。假设对于离散的随机变量，其全部可能的是为$x_1,x_2,...$，那么\n",
    "$$p(x_i)=P(X=x_i), i=1,2...$$\n",
    "也成为随机变量$X$的概率分布，表示为\n",
    "$$X\\sim p(x)$$\n",
    "读作$X$服从$p(x)$的概率分布。\n",
    "\n",
    "## 多维随机变量\n",
    "如果$X_1(\\omega), X_2(\\omega), ..., X_n(\\omega)$是定义在同一个样本空间下$\\Omega={\\omega}$下的n个随机变量，则称\n",
    "$$X(\\omega)=(X_1(\\omega),X_2(\\omega),\\cdots,X_n(\\omega))$$\n",
    "为n维随机变量或者随机向量。\n",
    "\n",
    "## 联合分布\n",
    "如果二维随机变量$(X, Y)$所有可能的取值为$(x_i, y_i),i,j=1,2,...$，并且两个随机变量同时发生的概率为：\n",
    "$$p_{ij}=P(X=x_i,Y=y_j)=P(X=x_i\\ 且\\ Y=y_j),\\quad i,j=1,2,\\cdots$$\n",
    "并且满足：\n",
    "- 非负性\n",
    "- 规范性和可加性，也就是累加和为1\n",
    "\n",
    "那么$P(X, Y)$就是联合概率密度函数(Joint Probability Mass Function)，也成为联合分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习的可行性\n",
    "\n",
    "## 重要的名词和概念\n",
    "### 上帝函数\n",
    "假设存在一个全能的函数$f$，不论输入什么特征向量，其总能给出正确的输出，那么该函数$f$就称为上帝函数。\n",
    "$$y_i=f(\\boldsymbol{x_i}),\\quad \\boldsymbol{x_i}\\in\\mathcal{X},y_i\\in\\mathcal{Y}$$\n",
    "也可以写成\n",
    "$$f:\\mathcal{X}\\to\\mathcal{Y}$$\n",
    "集合$\\mathcal{X}$包含上帝函数所有的输入，称为输入空间(input space)，集合$\\mathcal{Y}$称为输出空间(output space)。\n",
    "\n",
    "### 假设空间\n",
    "假设空间(hypothesis space)是所有可能的有输入空间到输出空间映射的集合，对于感知机来说，假设空间就是所有可能的超平面：\n",
    "$$\\mathcal{H}=\\{\\ h(\\boldsymbol{x})=\\operatorname{sign}(\\boldsymbol{w}\\cdot\\boldsymbol{x}+b)\\ \\}$$\n",
    "\n",
    "### 学习算法\n",
    "学习算法(learning algorithm)是从假设空间中挑选合适的映射的算法，对于感知机来说，有暴力算法、口袋算法等。\n",
    "\n",
    "学习模型$\\mathcal{M}$就是假设空间$\\mathcal{H}$+学习算法。\n",
    "\n",
    "### 泛化误差\n",
    "对于学习模型找到的函数$h$，需要评估其泛化能力，机器学习中使用泛化误差来评估泛化能力：\n",
    "$$R(h)=E_{(\\boldsymbol{x},y)\\sim\\mathcal{D}}[I(h(\\boldsymbol{x})\\ne y)]$$\n",
    "\n",
    "其中$I$为示性函数\n",
    "$$I(\\omega)=\n",
    "\\begin{cases}\n",
    "    1,& 事件\\ \\omega\\ 发生\\\\\n",
    "    0,& 事件\\ \\omega\\ 未发生\n",
    "\\end{cases}$$\n",
    "$\\mathcal{D}$是有上帝函数$f$产生的分布\n",
    "\n",
    "因此泛化误差$R(h)$的最大值为1，这是因为分布$\\mathcal{D}$的概率质量函数累加和为1，最大值的情况是$h$对于分布$\\mathcal{D}$的预测全是错误的，那么泛化误差就是概率质量函数累加和。\n",
    "\n",
    "### 经验误差\n",
    "泛化误差是在分布$\\mathcal{D}$上计算得到的，由于上帝函数$f$是未知的，因此分布$\\mathcal{D}$也是未知的，因此无法计算出来$R(h)$。为了评估函数$h$的泛化能力，我们引入经验误差，经验误差是在数据集$D$上计算得到的\n",
    "$$\\hat{R}_D(h)=\\frac{1}{n}\\sum_{i=1}^{n}I(h(\\boldsymbol{x_i})\\ne y_i)$$\n",
    "其中$I$是示性函数，$n$是数据集$D$中元素个数。\n",
    "\n",
    "这里需要注意，经验误差$\\hat{R}_D(h)$是在数据集$D$上计算得到的，可能和泛化误差$R(h)$是不相等的，但是我们假设数据集$D$是分布$\\mathcal{D}$的独立同分布，这样经验误差$\\hat{R}_D(h)$是泛化误差$R(h)$的无偏估计。用形象一点的语言来说，假如泛化误差是靶心，那么不同的数据集$D$计算出来的经验误差$\\hat{R}_D(h)$会在靶心附近变化，因此可以把经验误差$\\hat{R}_D(h)$当作泛化误差$R(h)$的近似。\n",
    "\n",
    "### 大概率近似正确(PAC)\n",
    "根据霍夫丁不等式：\n",
    "$$P\\left(\\left|\\hat{R}_D(h)-R(h)\\right| > \\epsilon \\right) \\le 2e^{-2\\epsilon^2 N}$$\n",
    "\n",
    "其中$N$是数据集的大小，这个不等式表明经验误差$\\hat{R}_D(h)$和泛化误差$R(h)$差值为$\\epsilon$的概率。\n",
    "\n",
    "当数据集$D$越大，那么$\\hat{R}_D(h)$和$R(h)$会大概率非常接近，使用机器学习的术语就是\n",
    "$$“\\hat{R}_D(h) = R(h)”\\ \\text{是 PAC 的}$$\n",
    "其中PAC(probability approximately correct)\n",
    "\n",
    "K-折交叉验证式在验证集上计算$\\hat{R}_D(h)$的，K折后计算一个平均值作为这个epoch的$\\hat{R}_D(h)$，相当于使用更大的数据集$D$来计算$\\hat{R}_D(h)$，因此能够更准确的反应泛化误差$R(h)$\n",
    "\n",
    "### 经验误差最小原则\n",
    "机器学习算法的设计原则就是，利用独立同分布的数据集$\\mathcal{D}$，用各种方法尽量去遍历假设空间$\\mathcal{H}$，从中选择使得$\\hat{R}_D(h)$最小的$h$作为最终的$g$，这就是经验误差最小原则(empirical rise minimization, ERM)\n",
    "$$g=\\operatorname*{argmin}_{h\\in\\mathcal{H}}\\hat{R}_D(h)$$\n",
    "\n",
    "对于大小为$M$的假设空间$\\mathcal{H}$，怎么确定找到的$g$是PAC的。令事件$A_i$是假设空间中第$i$元素是PAC的，即：\n",
    "$$A_i=(\\left|\\hat{R}_D(h_i)-R(h_i)\\right| > \\epsilon)$$\n",
    "那么有\n",
    "$$P(A_i)=P\\left(\\left|\\hat{R}_D(h_i)-R(h_i)\\right| > \\epsilon \\right) \\le 2e^{-2\\epsilon^2 N}$$\n",
    "假设$\\mathcal{H}$中所有元素都是PAC的，那么找到的$g$肯定也是PAC的，那么有\n",
    "$$P\\left(\\left|\\hat{R}_D(g)-R(g)\\right| > \\epsilon \\right)\\le P\\left(\\bigcup_{i=1}^{M}A_i\\right) \\le 2Me^{-2\\epsilon^2 N}$$\n",
    "\n",
    "因此只要数据集$D$足够大或者假设空间$\\mathcal{H}$是有限的，可以保证最终找的$g$是PAC的，也就是机器学习是可行的。\n",
    "\n",
    "### 成长函数、断点和VC维\n",
    "成长函数是描述假设空间$\\mathcal{H}$大小和数据集$D$大小的关系：\n",
    "$$\\text{Growth Function}=m_{\\mathcal{H}}(N)$$\n",
    "\n",
    "断点(break point)可以理解为函数断点，表示在某个$N$下，成长函数和$N-1$之前发生了突变，记为$k$，并且成长函数满足\n",
    "$$m_{\\mathcal{H}}(N)\\le N^{k-1},\\quad N\\ge 2, k\\ge 3$$\n",
    "\n",
    "那么有\n",
    "$$P\\left(\\left|\\hat{R}_D(g)-R(g)\\right| > \\epsilon \\right)\\le 4\\cdot (2N)^{k-1}\\cdot e^{-\\frac{1}{8}\\epsilon^2 N},N\\ 足够大$$\n",
    "固定$\\epsilon$和$k$后，上述是一个先递增后递减的函数，那么表明当数据集$D$足够大，保证最终找到$g$是PAC的。\n",
    "\n",
    "断点越大，说明模型越复杂，比如在二分类下，数据集$D$中所有可能的情况为$N^2$，如果断点为$k$，那么表示模型在数据集$D$大小为$k$时无法处理所有情况，即$k^2$，因此如果$k$越大，说明模型在数据集$D$大小小于$k$能够处理所有情况，因此$k$越大，说明模型能够处理的可能性越多，也就是模型越复杂。在机器学习中我们使用VC维(VC dimension)来描述模型的复杂度：\n",
    "$$d_{vc}=k-1$$\n",
    "\n",
    "### 模型的偏差和方差\n",
    "模型是\n",
    "$$模型\\mathcal{M} = 假设空间\\mathcal{H} + 算法\\mathcal{A}$$\n",
    "模型的偏差是模型的固有属性，是上帝函数$f$和假设空间$\\mathcal{H}$的距离，如果需要减小偏差，可以通过换一个模型也就是换一个假设空间来尝试。\n",
    "\n",
    "模型的方差是假设空间中误差最小的$h^*$实际学习到的$g$之间的距离，因此如果选择的模型较为复杂，也就是$d_{vc}$比较大，如果数据集$D$不够大，可能找到的$g$和$h^*$距离较远，也就是方差比较大。\n",
    "\n",
    "数据集D有限的情况下，机器学习的主要矛盾是，偏差减小方差会增大，偏差增加方差减小。用动画来表示就是，当假设空间远离f时，就会缩小；反之，则会增大。\n",
    "\n",
    "### 欠拟合和过拟合\n",
    "- 欠拟合表示在训练集和验证集上的误差都比较大，解决方案是尝试其他模型来减少偏差\n",
    "- 过拟合表示在训练集上误差比较小，在验证集上误差比较大，解决方案是减少方差，比如尝试简单点的模型"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
