{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import graphviz\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 课程准备\n",
    "\n",
    "## PyTorch基本使用\n",
    "\n",
    "### 什么是张量\n",
    "\n",
    "张量(tensor)是一个由数值构成的数组，可以有多个维度。\n",
    "\n",
    "构建一个一维数组，其包含12个`int64`类型的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([12]), number element: 12, type of element: torch.int64\n",
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n"
     ]
    }
   ],
   "source": [
    "x: torch.Tensor = torch.arange(12)\n",
    "print(f\"shape: {x.shape}, number element: {x.numel()}, type of element: {x.dtype}\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用`reshape`改变一个张量的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:  torch.Size([3, 4])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "x = x.reshape(3, 4)\n",
    "print(\"shape: \", x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用的张量操作\n",
    "\n",
    "#### cat\n",
    "可以使用`cat`将多个有着相同形状(shape)的张量拼接起来，其中`dim`表示拼接的维度\n",
    "\n",
    "比如现在有两个维度为$n$的张量$X$和$Y$，`dim`可取的范围为$[-n, n-1]$\n",
    "\n",
    "$$\n",
    "cat((X, Y), dim=d)=\\begin{cases}\n",
    "cat((X, Y), dim=d+n) &, d \\lt 0 \\\\\\\\\n",
    "X \\ \\mathbf{join} \\ Y &, d=0 \\\\\\\\\n",
    "X[i_0][i_1]\\cdots[i_{d-1}]+Y[i_0][i_1]\\cdots[i_{d-1}], i_k \\in \\{0, 1, 2, \\cdots X.shape[k]-1\\} &, d > 0\n",
    "\\end{cases}$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$Z=X\\ \\mathbf{join} \\ Y=[X[0], X[1], \\cdots, Y[0], Y[1], \\cdots]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.]],\n",
      "\n",
      "        [[ 6.,  7.,  8.],\n",
      "         [ 9., 10., 11.]]])\n",
      " Y=tensor([[[12., 13., 14.],\n",
      "         [15., 16., 17.]],\n",
      "\n",
      "        [[18., 19., 20.],\n",
      "         [21., 22., 23.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2., 12., 13., 14.],\n",
       "         [ 3.,  4.,  5., 15., 16., 17.]],\n",
       "\n",
       "        [[ 6.,  7.,  8., 18., 19., 20.],\n",
       "         [ 9., 10., 11., 21., 22., 23.]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape((2, 2, 3))\n",
    "Y = torch.arange(12, 24, dtype=torch.float32).reshape((2, 2, 3))\n",
    "print(f\"X={X}\\n Y={Y}\")\n",
    "torch.cat((X, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sum\n",
    "对一个张量执行求和操作，包含`axis`表示沿着该轴求和，其中$axis \\in \\{-ndim, -ndim-1, \\cdots, -1, 0, 1, \\cdots,ndim-1\\}$，其中$ndim$是张量的维度。\n",
    "\n",
    "默认`axis=None`，此时是求张量所有元素的和，返回的是一个标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(276), 276.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(24).reshape(2, 3, 4)\n",
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当`axis`是整数时，和`cat`一样，如果$axis \\lt 0$，相当于$axis+ndim$。对于任意的$axis \\ge 0$其结果为\n",
    "\n",
    "$$\\sum_{i=0}^{X.shape[axis-1]-1}X[:, \\cdots, i, :, \\cdots]$$\n",
    "\n",
    "其中$i$所在的维度是`axis`表示的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "print(X.sum(axis=0) == X[0, :, :] + X[1, :, :])\n",
    "print(X.sum(axis=1) == X[:, 0, :] + X[:, 1, :] + X[:, 2, :])\n",
    "print(X.sum(axis=2) == X[:, :, 0]+X[:, :, 1]+X[:, :, 2]+X[:, :, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`axis`也可以包含多个值\n",
    "\n",
    "$$\\sum_{i_1=0}\\cdots\\sum_{i_k=0}X[:,\\cdots,i_1,:,\\cdots,:,i_k,\\cdots,:]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "source": [
    "expect =torch.zeros(4)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        expect += X[i, j, :]\n",
    "print(X.sum(axis=(0,1)) == expect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 广播(broadcasting)\n",
    "\n",
    "在正常情况，对两个张量执行任意操作的前提是两个张量有一样的形状。但是PyTorch允许两个张量即使形状不同也可以执行，这种机制称为[广播](https://numpy.org/doc/stable/user/basics.broadcasting.html#basics-broadcasting)。\n",
    "\n",
    "```python\n",
    ">>> a = np.array([1.0, 2.0, 3.0])\n",
    ">>> b = np.array([2.0, 2.0, 2.0])\n",
    ">>> a * b\n",
    "array([2.,  4.,  6.])\n",
    "```\n",
    "\n",
    "`PyTorch`在执行张量的操作之前会检查两个张量的形状，从右往左一个一个元素比较\n",
    "\n",
    "$$\\begin{aligned}\n",
    "A: a_0 \\times &a_1 \\cdots \\times a_n \\\\\n",
    "B: \\ \\ \\ \\ \\ \\ \\ \\ &b_0 \\cdots \\times b_n\n",
    "\\end{aligned}$$\n",
    "\n",
    "只有下面两种情况，检查才会成功，否则会抛出`ValueError`的异常\n",
    "- 元素相等\n",
    "- 其中一个是1\n",
    "\n",
    "当检查通过后，但是张量形状不一致时，会将张量广播成形状一致的张量，比如\n",
    "\n",
    "$$\\begin{aligned}\n",
    "A&: 8 \\times 1 \\times 6 \\times 1 \\\\\n",
    "B&: \\ \\ \\ \\ \\ \\ \\ 7 \\times 1 \\times 5 \\\\\n",
    "Result&: 8\\times7\\times6\\times5\n",
    "\\end{aligned}$$\n",
    "\n",
    "对于需要广播的张量，比如维度$d_{old}$需要广播到$d_{new}$，那么会将$\\{d_{old+1}, d_{old+2}, \\cdots, d_{old+n-1}\\}$复制$d_{new}-1$个\n",
    "\n",
    "比如\n",
    "```python\n",
    ">>> a = np.array([[ 0.0,  0.0,  0.0],\n",
    "...               [10.0, 10.0, 10.0],\n",
    "...               [20.0, 20.0, 20.0],\n",
    "...               [30.0, 30.0, 30.0]])\n",
    ">>> b = np.array([1.0, 2.0, 3.0])\n",
    ">>> a + b\n",
    "array([[  1.,   2.,   3.],\n",
    "        [11.,  12.,  13.],\n",
    "        [21.,  22.,  23.],\n",
    "        [31.,  32.,  33.]])\n",
    ">>> b = np.array([1.0, 2.0, 3.0, 4.0])\n",
    ">>> a + b\n",
    "Traceback (most recent call last):\n",
    "ValueError: operands could not be broadcast together with shapes (4,3) (4,)\n",
    "```\n",
    "\n",
    "![](../resources/broadcasting_1.png)\n",
    "\n",
    "![](../resources/broadcasting_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "b=tensor([[0, 1]])\n",
      "broadcasted_a=tensor([[0, 0],\n",
      "        [1, 1],\n",
      "        [2, 2]])\n",
      "broadcasted_b=tensor([[0, 1],\n",
      "        [0, 1],\n",
      "        [0, 1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape(3, 1)\n",
    "b = torch.arange(2).reshape(1, 2)\n",
    "print(f\"a={a}\\nb={b}\\nbroadcasted_a={torch.cat((a, a), dim=1)}\\nbroadcasted_b={torch.cat((b, b, b))}\")\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导\n",
    "\n",
    "### 矩阵求导\n",
    "\n",
    "考虑一个函数\n",
    "\n",
    "$$function(input)$$\n",
    "\n",
    "其中$input$和$function$可以是标量、向量和矩阵中的任意一种，因此总共有六种组合。\n",
    "\n",
    "矩阵求导就是针对不同类型的$input$和$function$，能够给出统一形式求解偏导数的一种规则。\n",
    "\n",
    "标量可以看作维度为1的向量，因此标量、向量可以统一处理。而对于矩阵的情况，需要先将矩阵转为向量，称之为向量化。因此不论标量、向量和矩阵都可以统一视为向量来处理。\n",
    "\n",
    "$$\\mathbf{vec}(\\boldsymbol{X}_{m \\times n})=\\begin{bmatrix}\n",
    "x_{11} \\\\\n",
    "x_{21} \\\\\n",
    "\\vdots \\\\\n",
    "x_{m1} \\\\\n",
    "\\vdots \\\\\n",
    "x_{1n} \\\\\n",
    "x_{2n} \\\\\n",
    "\\vdots \\\\\n",
    "x_{mn}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "### 雅可比矩阵\n",
    "\n",
    "借助雅可比矩阵可以对$\\mathbb{R}^m\\to\\mathbb{R}^n$的函数求解其偏导数矩阵。\n",
    "\n",
    "假设$f_1, f_2, \\cdots, f_n$是关于$x_1, x_2, \\cdots, x_m$的函数，并且每个函数对于各个自变量的偏导数都存在，定义如下函数矩阵为雅可比矩阵(Jacobian Matrix)\n",
    "\n",
    "$$\n",
    "J(x_1, x_2, \\cdots, x_m)=\\frac{\\partial (f_1, f_2, \\cdots, f_n)}{\\partial (x_1, x_2, \\cdots, x_m)}=\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1}& \\frac{\\partial f_1}{\\partial x_2}& \\cdots& \\frac{\\partial f_1}{\\partial x_m} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1}& \\frac{\\partial f_2}{\\partial x_2}& \\cdots& \\frac{\\partial f_2}{\\partial x_m} \\\\\n",
    "\\vdots& \\vdots& \\vdots& \\vdots \\\\\n",
    "\\frac{\\partial f_n}{\\partial x_1}& \\frac{\\partial f_n}{\\partial x_2}& \\cdots& \\frac{\\partial f_n}{\\partial x_m} \\\\\n",
    "\\end{bmatrix}_{n \\times m}\n",
    "$$\n",
    "\n",
    "### 链式法则\n",
    "对于$\\boldsymbol{f}(\\boldsymbol{w})$，其中$\\boldsymbol{f}$是$n$维向量，$\\boldsymbol{w}$是$p$维向量，并且$\\boldsymbol{w}=\\boldsymbol{w}(\\boldsymbol{x})$，其中$\\boldsymbol{x}$为$m$维向量，借助雅可比矩阵和矩阵乘法，有\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial (\\boldsymbol{f})}{\\partial (\\boldsymbol{x})} \n",
    "&= \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1}& \\frac{\\partial f_1}{\\partial x_2}& \\cdots& \\frac{\\partial f_1}{\\partial x_m} \\\\\n",
    "\\frac{\\partial f_2}{\\partial x_1}& \\frac{\\partial f_2}{\\partial x_2}& \\cdots& \\frac{\\partial f_2}{\\partial x_m} \\\\\n",
    "\\vdots& \\vdots& \\vdots& \\vdots \\\\\n",
    "\\frac{\\partial f_n}{\\partial x_1}& \\frac{\\partial f_n}{\\partial x_2}& \\cdots& \\frac{\\partial f_n}{\\partial x_m} \\\\\n",
    "\\end{bmatrix}_{n \\times m} \\\\\\\\\n",
    "\n",
    "&= \\begin{bmatrix}\n",
    "\\sum_{i=1}^{p}\\frac{\\partial f_1}{\\partial w_i}\\frac{\\partial w_i}{\\partial x_1}& \\sum_{i=1}^{p}\\frac{\\partial f_1}{\\partial w_i}\\frac{\\partial w_i}{\\partial x_2}& \\cdots& \\sum_{i=1}^{p}\\frac{\\partial f_1}{\\partial w_i}\\frac{\\partial w_i}{\\partial x_m} \\\\\n",
    "\\sum_{i=1}^{p}\\frac{\\partial f_2}{\\partial w_i}\\frac{\\partial w_i}{\\partial x_1}& \\sum_{i=1}^{p}\\frac{\\partial f_2}{\\partial w_i}\\frac{\\partial w_i}{\\partial x_2}& \\cdots& \\sum_{i=1}^{p}\\frac{\\partial f_2}{\\partial w_i}\\frac{\\partial w_i}{\\partial x_m} \\\\\n",
    "\\vdots& \\vdots& \\vdots& \\vdots \\\\\n",
    "\\sum_{i=1}^{p}\\frac{\\partial f_n}{\\partial w_i}\\frac{\\partial w_i}{\\partial x_1}& \\sum_{i=1}^{p}\\frac{\\partial f_n}{\\partial w_i}\\frac{\\partial w_i}{\\partial x_2}& \\cdots& \\sum_{i=1}^{p}\\frac{\\partial f_n}{\\partial w_i}\\frac{\\partial w_i}{\\partial x_m} \\\\\n",
    "\\end{bmatrix}_{n \\times m} \\\\\\\\\n",
    "\n",
    "&= \\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial w_1}& \\frac{\\partial f_1}{\\partial w_2}& \\cdots& \\frac{\\partial f_1}{\\partial w_p}\\\\\n",
    "\\frac{\\partial f_2}{\\partial w_1}& \\frac{\\partial f_2}{\\partial w_2}& \\cdots& \\frac{\\partial f_2}{\\partial w_p}\\\\\n",
    "\\vdots& \\vdots& \\vdots& \\vdots \\\\\n",
    "\\frac{\\partial f_n}{\\partial w_1}& \\frac{\\partial f_n}{\\partial w_2}& \\cdots& \\frac{\\partial f_n}{\\partial w_p}\\\\\n",
    "\\end{bmatrix}_{n \\times p} \\cdot \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial w_1}{\\partial x_1}& \\frac{\\partial w_1}{\\partial x_2}& \\cdots& \\frac{\\partial w_1}{\\partial x_m} \\\\\n",
    "\\frac{\\partial w_2}{\\partial x_1}& \\frac{\\partial w_2}{\\partial x_2}& \\cdots& \\frac{\\partial w_2}{\\partial x_m} \\\\\n",
    "\\vdots& \\vdots& \\vdots& \\vdots \\\\\n",
    "\\frac{\\partial w_p}{\\partial x_1}& \\frac{\\partial w_p}{\\partial x_2}& \\cdots& \\frac{\\partial w_p}{\\partial x_m} \\\\\n",
    "\\end{bmatrix}_{p \\times m} \\\\\\\\\n",
    "\n",
    "&= \\frac{\\partial (\\boldsymbol{f})}{\\partial (\\boldsymbol{w})}\\frac{\\partial (\\boldsymbol{w})}{\\partial (\\boldsymbol{x})}\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "上述$\\boldsymbol{w}$称为中间变量，链式法则还可以推广到包含多个中间变量，设$\\boldsymbol{w_i}$为第$i$个中间变量，并且总共有$q$个中间变量，那么有\n",
    "\n",
    "$$\\frac{\\partial (\\boldsymbol{f})}{\\partial (\\boldsymbol{x})}=\\frac{\\partial (\\boldsymbol{f})}{\\partial (\\boldsymbol{w_q})}\\frac{\\partial (\\boldsymbol{w_{q}})}{\\partial (\\boldsymbol{w_{q-1}})}\\cdots\\frac{\\partial (\\boldsymbol{w_2})}{\\partial (\\boldsymbol{w_1})}\\frac{\\partial (\\boldsymbol{w_1})}{\\partial (\\boldsymbol{x})}$$\n",
    "\n",
    "### 自动求导\n",
    "\n",
    "在大部分机器学习的算法，都是对某个损失函数$L$优化，得到使得损失函数$L$值最小的模型参数。在监督学习中的[逻辑回归](../SupervisedLearning/logistic_regression.ipynb)中我们介绍过梯度下降法，通过计算损失函数的$L$的梯度，同时梯度的反方向是函数值下降最快的方向来得到损失函数$L$的最小值，完成学习。\n",
    "\n",
    "运用梯度下降法的关键是计算一个函数在给定变量上的偏导数（梯度），在[逻辑回归](../SupervisedLearning/logistic_regression.ipynb)中式通过显示定义经验误差的导函数`drhd`来计算梯度的。\n",
    "\n",
    "自动求导是计算梯度的一种机制，可以避免手动写导函数的代码，其主要原理是链式法则。\n",
    "\n",
    "自动求导实现的关键是计算图，计算图定义了`tensor`的运算步骤，`tensor`的计算可以分成两种情况\n",
    "- 二元运算，比如加法、减法等需要两个变量的运算\n",
    "- 一元运算，比如取负、乘方、三角函数等只需要一个变量\n",
    "\n",
    "因此可以把一个运算表达式拆成多个中间变量，每次运算最多只有两个中间变量参与运算，比如\n",
    "\n",
    "$$f(x_1, x_2)=ln(x_1)+x_1x_2-sin(x_2)$$\n",
    "\n",
    "计算图如下，其中$v_i, 0 \\lt i \\lt 5$都是中间变量\n",
    "\n",
    "![](../resources/autograd2.png)\n",
    "\n",
    "接下来根据计算图可以计算偏导数（梯度）了，有两种计算方式：前向累积和反向传播。\n",
    "\n",
    "以如下函数来解释下两种计算模式的区别\n",
    "\n",
    "$$\\begin{cases}\n",
    "f(x_1, x_2) &= ln(x_1)+x_1x_2-sin(x_2) \\\\\n",
    "g(x_1, x_2) &= x_1x_2 - x_2 + x_1^2\n",
    "\\end{cases}$$\n",
    "\n",
    "计算图如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"424pt\" height=\"276pt\"\n",
       " viewBox=\"0.00 0.00 424.38 276.40\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 272.4)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-272.4 420.38,-272.4 420.38,4 -4,4\"/>\n",
       "<!-- x1 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>x1</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"22.1\" cy=\"-91.7\" rx=\"22.2\" ry=\"22.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"22.1\" y=\"-88\" font-family=\"Times,serif\" font-size=\"14.00\">x1</text>\n",
       "</g>\n",
       "<!-- ln(x1) -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>ln(x1)</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"104.89\" cy=\"-91.7\" rx=\"24.9\" ry=\"24.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"104.89\" y=\"-88\" font-family=\"Times,serif\" font-size=\"14.00\">w1</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;ln(x1) -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>x1&#45;&gt;ln(x1)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M44.33,-91.7C52.1,-91.7 61.12,-91.7 69.74,-91.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"69.96,-95.2 79.96,-91.7 69.96,-88.2 69.96,-95.2\"/>\n",
       "</g>\n",
       "<!-- x1x2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>x1x2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"104.89\" cy=\"-158.7\" rx=\"24.9\" ry=\"24.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"104.89\" y=\"-155\" font-family=\"Times,serif\" font-size=\"14.00\">w2</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;x1x2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>x1&#45;&gt;x1x2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M39.84,-105.58C50.68,-114.57 65.04,-126.48 77.41,-136.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"75.27,-139.5 85.2,-143.19 79.73,-134.12 75.27,-139.5\"/>\n",
       "</g>\n",
       "<!-- x1^2 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>x1^2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"104.89\" cy=\"-24.7\" rx=\"24.9\" ry=\"24.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"104.89\" y=\"-21\" font-family=\"Times,serif\" font-size=\"14.00\">w6</text>\n",
       "</g>\n",
       "<!-- x1&#45;&gt;x1^2 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>x1&#45;&gt;x1^2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M39.84,-77.82C50.68,-68.82 65.04,-56.92 77.41,-46.66\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"79.73,-49.28 85.2,-40.2 75.27,-43.89 79.73,-49.28\"/>\n",
       "</g>\n",
       "<!-- x2 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>x2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"22.1\" cy=\"-211.7\" rx=\"22.2\" ry=\"22.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"22.1\" y=\"-208\" font-family=\"Times,serif\" font-size=\"14.00\">x2</text>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;x1x2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>x2&#45;&gt;x1x2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M41.03,-199.94C51.11,-193.33 63.92,-184.92 75.34,-177.43\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"77.3,-180.33 83.74,-171.92 73.46,-174.48 77.3,-180.33\"/>\n",
       "</g>\n",
       "<!-- sin(x2) -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>sin(x2)</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"190.29\" cy=\"-243.7\" rx=\"24.9\" ry=\"24.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.29\" y=\"-240\" font-family=\"Times,serif\" font-size=\"14.00\">w3</text>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;sin(x2) -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>x2&#45;&gt;sin(x2)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43.63,-217.14C54.46,-219.9 68,-223.2 80.2,-225.7 105.41,-230.86 134.13,-235.55 155.77,-238.85\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"155.38,-242.33 165.78,-240.35 156.42,-235.41 155.38,-242.33\"/>\n",
       "</g>\n",
       "<!-- x1x2&#45;x2 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>x1x2&#45;x2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"190.29\" cy=\"-164.7\" rx=\"24.9\" ry=\"24.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.29\" y=\"-161\" font-family=\"Times,serif\" font-size=\"14.00\">w7</text>\n",
       "</g>\n",
       "<!-- x2&#45;&gt;x1x2&#45;x2 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>x2&#45;&gt;x1x2&#45;x2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M44.3,-209.57C66.08,-207.01 100.77,-201.85 129.59,-192.7 139.47,-189.56 149.83,-185.14 159.09,-180.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"160.86,-183.77 168.28,-176.21 157.77,-177.49 160.86,-183.77\"/>\n",
       "</g>\n",
       "<!-- ln(x1)+x1x2 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>ln(x1)+x1x2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"190.29\" cy=\"-97.7\" rx=\"24.9\" ry=\"24.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"190.29\" y=\"-94\" font-family=\"Times,serif\" font-size=\"14.00\">w4</text>\n",
       "</g>\n",
       "<!-- ln(x1)&#45;&gt;ln(x1)+x1x2 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>ln(x1)&#45;&gt;ln(x1)+x1x2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M129.57,-93.4C137.58,-93.98 146.69,-94.63 155.34,-95.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"155.33,-98.76 165.56,-95.99 155.83,-91.78 155.33,-98.76\"/>\n",
       "</g>\n",
       "<!-- x1x2&#45;&gt;ln(x1)+x1x2 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>x1x2&#45;&gt;ln(x1)+x1x2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M125.24,-144.55C136,-136.68 149.59,-126.74 161.47,-118.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"163.76,-120.71 169.76,-111.98 159.63,-115.06 163.76,-120.71\"/>\n",
       "</g>\n",
       "<!-- x1x2&#45;&gt;x1x2&#45;x2 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>x1x2&#45;&gt;x1x2&#45;x2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M129.57,-160.4C137.58,-160.98 146.69,-161.63 155.34,-162.26\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"155.33,-165.76 165.56,-162.99 155.83,-158.78 155.33,-165.76\"/>\n",
       "</g>\n",
       "<!-- ln(x1)+x1x2&#45;sin(x2) -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>ln(x1)+x1x2&#45;sin(x2)</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"275.69\" cy=\"-194.7\" rx=\"24.9\" ry=\"24.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"275.69\" y=\"-191\" font-family=\"Times,serif\" font-size=\"14.00\">w5</text>\n",
       "</g>\n",
       "<!-- sin(x2)&#45;&gt;ln(x1)+x1x2&#45;sin(x2) -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>sin(x2)&#45;&gt;ln(x1)+x1x2&#45;sin(x2)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M211.9,-231.59C221.93,-225.7 234.16,-218.51 245.15,-212.05\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"247.17,-214.92 254.02,-206.84 243.62,-208.89 247.17,-214.92\"/>\n",
       "</g>\n",
       "<!-- ln(x1)+x1x2&#45;&gt;ln(x1)+x1x2&#45;sin(x2) -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>ln(x1)+x1x2&#45;&gt;ln(x1)+x1x2&#45;sin(x2)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M206.98,-115.94C219.71,-130.75 237.83,-151.83 252.12,-168.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"249.61,-170.9 258.79,-176.2 254.92,-166.34 249.61,-170.9\"/>\n",
       "</g>\n",
       "<!-- f(x1, x2) -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>f(x1, x2)</title>\n",
       "<text text-anchor=\"middle\" x=\"376.38\" y=\"-191\" font-family=\"Times,serif\" font-size=\"14.00\">f(x1, x2)</text>\n",
       "</g>\n",
       "<!-- ln(x1)+x1x2&#45;sin(x2)&#45;&gt;f(x1, x2) -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>ln(x1)+x1x2&#45;sin(x2)&#45;&gt;f(x1, x2)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.58,-194.7C308.87,-194.7 318.49,-194.7 328.01,-194.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"328.23,-198.2 338.23,-194.7 328.23,-191.2 328.23,-198.2\"/>\n",
       "</g>\n",
       "<!-- x1x2&#45;x2+x1^2 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>x1x2&#45;x2+x1^2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"275.69\" cy=\"-97.7\" rx=\"24.9\" ry=\"24.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"275.69\" y=\"-94\" font-family=\"Times,serif\" font-size=\"14.00\">w8</text>\n",
       "</g>\n",
       "<!-- x1^2&#45;&gt;x1x2&#45;x2+x1^2 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>x1^2&#45;&gt;x1x2&#45;x2+x1^2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M128.92,-31.64C151.32,-38.69 186.11,-50.43 214.99,-63.7 225.25,-68.41 236.1,-74.29 245.68,-79.83\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"244.04,-82.93 254.43,-84.99 247.59,-76.9 244.04,-82.93\"/>\n",
       "</g>\n",
       "<!-- x1x2&#45;x2&#45;&gt;x1x2&#45;x2+x1^2 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>x1x2&#45;x2&#45;&gt;x1x2&#45;x2+x1^2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M210.21,-149.49C221.3,-140.59 235.49,-129.19 247.73,-119.35\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"250.23,-121.83 255.84,-112.84 245.85,-116.38 250.23,-121.83\"/>\n",
       "</g>\n",
       "<!-- g(x1, x2) -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>g(x1, x2)</title>\n",
       "<text text-anchor=\"middle\" x=\"376.38\" y=\"-94\" font-family=\"Times,serif\" font-size=\"14.00\">g(x1, x2)</text>\n",
       "</g>\n",
       "<!-- x1x2&#45;x2+x1^2&#45;&gt;g(x1, x2) -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>x1x2&#45;x2+x1^2&#45;&gt;g(x1, x2)</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M300.58,-97.7C308.37,-97.7 317.33,-97.7 326.28,-97.7\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"326.31,-101.2 336.31,-97.7 326.31,-94.2 326.31,-101.2\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f2b4742f190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = graphviz.Digraph()\n",
    "G.attr(rankdir=\"LR\")\n",
    "G.attr(\"node\", shape=\"circle\")\n",
    "G.node(\"x1\", label=\"x1\")\n",
    "G.node(\"x2\", label=\"x2\")\n",
    "G.node(\"ln(x1)\", label=\"w1\")\n",
    "G.node(\"x1x2\", label=\"w2\")\n",
    "G.node(\"sin(x2)\", label=\"w3\")\n",
    "G.node(\"ln(x1)+x1x2\", label=\"w4\")\n",
    "G.node(\"ln(x1)+x1x2-sin(x2)\", label=\"w5\")\n",
    "G.node(\"f(x1, x2)\", shape=\"plaintext\")\n",
    "G.node(\"x1^2\", label=\"w6\")\n",
    "G.node(\"x1x2-x2\", label=\"w7\")\n",
    "G.node(\"x1x2-x2+x1^2\", label=\"w8\")\n",
    "G.node(\"g(x1, x2)\", shape=\"plaintext\")\n",
    "\n",
    "G.edge(\"x1\", \"ln(x1)\")\n",
    "G.edge(\"x1\", \"x1x2\"), G.edge(\"x2\", \"x1x2\")\n",
    "G.edge(\"x2\", \"sin(x2)\")\n",
    "G.edge(\"ln(x1)\", \"ln(x1)+x1x2\"), G.edge(\"x1x2\", \"ln(x1)+x1x2\")\n",
    "G.edge(\"ln(x1)+x1x2\", \"ln(x1)+x1x2-sin(x2)\"), G.edge(\"sin(x2)\", \"ln(x1)+x1x2-sin(x2)\")\n",
    "G.edge(\"ln(x1)+x1x2-sin(x2)\", \"f(x1, x2)\")\n",
    "G.edge(\"x1\", \"x1^2\")\n",
    "G.edge(\"x1x2\", \"x1x2-x2\"), G.edge(\"x2\", \"x1x2-x2\")\n",
    "G.edge(\"x1^2\", \"x1x2-x2+x1^2\"), G.edge(\"x1x2-x2\", \"x1x2-x2+x1^2\")\n",
    "G.edge(\"x1x2-x2+x1^2\", \"g(x1, x2)\")\n",
    "G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前向累积\n",
    "\n",
    "计算变量$x_1$的偏导数为例，需要计算\n",
    "$$\\frac{\\partial f}{\\partial x_1} , \\ \\frac{\\partial g}{\\partial x_1}$$\n",
    "\n",
    "根据链式法则有\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial f}{\\partial x_1} &= (\\frac{\\partial w_1}{\\partial x_1}\\frac{\\partial w_4}{\\partial w_1} + \\frac{\\partial w_2}{\\partial x_1}\\frac{\\partial w_4}{\\partial w_2})\\frac{\\partial w_5}{\\partial w_4}\\frac{\\partial f}{\\partial w_5} \\\\\\\\\n",
    "\n",
    "\\frac{\\partial g}{\\partial x_1} &= (\\frac{\\partial w_2}{\\partial x_1}\\frac{\\partial w_7}{\\partial w_2}\\frac{\\partial w_8}{\\partial w_7}+\\frac{\\partial w_6}{\\partial x_1}\\frac{\\partial w_8}{\\partial w_6})\\frac{\\partial g}{\\partial w_8}\n",
    "\\end{aligned}$$\n",
    "\n",
    "定义$\\dot{w}_i$为$\\frac{\\partial w_i}{\\partial x_1}$，前向累积计算在$(x_1, x_2)=(2, 5)$的梯度的过程如下\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\hline\n",
    "&\\dot{x_1} = 1\\\\\n",
    "&\\dot{x_2} = 0 \\\\\n",
    "\\hline \\\\\n",
    "&\\dot{w_1} = \\frac{1}{{ln(x_1)}'} \\times \\dot{x_1} = \\frac{1}{2} \\\\\n",
    "&\\dot{w_2} = \\dot{x_1} \\times x_2+x_1 \\times \\dot{x_2} = 5 \\\\\n",
    "&\\dot{w_3} = cos(x_2) \\times \\dot{x_2} =0 \\\\\n",
    "&\\dot{w_4} = \\dot{w_1} + \\dot{w_2} = 5.5 \\\\\n",
    "&\\dot{w_5} = \\dot{w_4} - \\dot{w_3} = 5.5 \\\\\n",
    "&\\dot{w_6} = 2x_1 \\times \\dot{x_1} = 4 \\\\\n",
    "&\\dot{w_7} = \\dot{w_2} - \\dot{x_2} = 5 \\\\\n",
    "&\\dot{w_8} = \\dot{w_6} + \\dot{w_7} = 9 \\\\\n",
    "\\hline\n",
    "&\\dot{f} = \\dot{w_5} = 5.5 \\\\\n",
    "&\\dot{g} = \\dot{w_8} = 9 \n",
    "\\end{aligned}$$\n",
    "\n",
    "上面的计算过程，对于输入$(x_1, x_2)$，如果期望求$x_1$的偏导数，那么就将$\\dot{x_1}$设置为$1$，其余变量设置为$0$。接下来从计算图的$x_1$出发，计算每个中间变量$\\dot{w_i}$，前向累积结束后，可以得到所有输出对$x_1$的偏导数。\n",
    "\n",
    "上面的例子其雅可比矩阵为\n",
    "\n",
    "$$J=\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}& \\frac{\\partial f}{\\partial x_2} \\\\\n",
    "\\frac{\\partial g}{\\partial x_1}& \\frac{\\partial g}{\\partial x_2}\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "那么上述前向累积的过程可以表示为\n",
    "\n",
    "$$\n",
    "J\\dot{\\boldsymbol{x}}=\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}& \\frac{\\partial f}{\\partial x_2} \\\\\n",
    "\\frac{\\partial g}{\\partial x_1}& \\frac{\\partial g}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\frac{\\partial g}{\\partial x_1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "更一般地，对于任意方向$\\vec{r}$，计算函数$f_1, ..., f_m$在$\\vec{r}$的方向导数为\n",
    "\n",
    "$$\n",
    "J\\vec{r}=\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1}& \\cdots& \\frac{\\partial f_1}{\\partial x_n} \\\\\n",
    "\\vdots& \\vdots& \\vdots \\\\\n",
    "\\frac{\\partial f_m}{\\partial x_1}& \\cdots& \\frac{\\partial f_m}{\\partial x_n}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "r_1 \\\\\n",
    "\\vdots \\\\\n",
    "r_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "这也称为雅可比向量积(Jacobian-vector products)。\n",
    "\n",
    "因此如果需要求解梯度向量，对于$n$维$\\boldsymbol{x}$向量，总共需要$n$次雅可比向量积。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 反向传播\n",
    "\n",
    "反向传播和前向传播相反，还是看上面的例子，现在计算$\\frac{\\partial f}{\\partial \\boldsymbol{x}}$的梯度看下反向传播的计算过程，使用$\\bar{w_i}$表示$\\frac{\\partial f}{\\partial w_i}$\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\hline\n",
    "&\\bar{f} = 1 \\\\\n",
    "&\\bar{g} = 0 \\\\\n",
    "\\hline \n",
    "&\\bar{w_5}=\\bar{f}\\frac{\\partial f}{\\partial w_5}=1 \\\\\n",
    "&\\bar{w_4}=\\bar{w_5}\\frac{\\partial w_5}{\\partial w_4}=1 \\\\\n",
    "&\\bar{w_3}=\\bar{w_5}\\frac{\\partial w_5}{\\partial w_3}=-1 \\\\\n",
    "&\\bar{w_2}=\\bar{w_4}\\frac{\\partial w_4}{\\partial w_2}=1 \\\\\n",
    "&\\bar{w_1}=\\bar{w_4}\\frac{\\partial w_4}{\\partial w_1}=1 \\\\\n",
    "\\hline\n",
    "&\\bar{x_2}=\\bar{w_3}\\frac{\\partial w_3}{\\partial x_2}+\\bar{w_2}\\frac{\\partial w_2}{\\partial x_2}=1.716 \\\\\n",
    "&\\bar{x_1}=\\bar{w_1}\\frac{\\partial w_1}{\\partial x_1}+\\bar{w_2}\\frac{\\partial w_2}{\\partial x_1}=5.5\n",
    "\n",
    "\\end{aligned}$$\n",
    "\n",
    "一次反向传播能够计算$f$的梯度，使用雅可比向量积表示就是\n",
    "\n",
    "$$\n",
    "J^\\top\\bar{\\boldsymbol{f}}=\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}& \\frac{\\partial g}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f}{\\partial x_1}& \\frac{\\partial g}{\\partial x_1}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial f} \\\\\n",
    "\\frac{\\partial f}{\\partial g}\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1} \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "更一般地，对于函数$l(x_1, x_2, \\cdots, x_n)$，其中$l$有$m$个中间变量$f_1, f_2, \\cdots, f_m$，如果需要计算$l$地梯度，可以通过雅可比矩阵计算得出\n",
    "$$\n",
    "J^\\top\\vec{r}=\\begin{bmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1}& \\cdots& \\frac{\\partial f_m}{\\partial x_1} \\\\\n",
    "\\vdots& \\vdots& \\vdots \\\\\n",
    "\\frac{\\partial f_1}{\\partial x_n}& \\cdots& \\frac{\\partial f_m}{\\partial x_n} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial l}{\\partial f_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial l}{\\partial f_m}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "那么反向传播总共需要$m$次雅可比向量积就能够计算所有梯度了。\n",
    "\n",
    "#### 前向累积和反向传播对比\n",
    "1. 对于有$n$个自变量，$m$个因变量，现在需要计算所有因变量的梯度，那么前向累积需要$n$次雅可比向量积才能完成，反向传播需要$m$次雅可比向量积才能完成。\n",
    "2. 反向传播需要保存中间变量的值，比如\n",
    "$$\n",
    "\\frac{\\partial l}{\\partial w_1}=\\frac{\\partial l}{\\partial w_2}\\frac{\\partial w_2}{\\partial w_1}\n",
    "$$\n",
    "此时需要将中间变量$w_1$保存下来，才能计算出$\\frac{\\partial w_2}{\\partial w_1}$，因此反向传播更耗费内存资源。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2.0, 5.0)=11.652071952819824, gradient=[5.5, 1.71633780002594]\n",
      "g(2.0, 5.0)=9.0, gradient=[9.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "def f(x1, x2):\n",
    "    return torch.log(x1) + x1 @ x2 - torch.sin(x2)\n",
    "\n",
    "def g(x1, x2):\n",
    "    return x1 @ x2 - x2 + torch.pow(x1, 2)\n",
    "\n",
    "x1 = torch.tensor([2.0], requires_grad=True)\n",
    "x2 = torch.tensor([5.0], requires_grad=True)\n",
    "y = torch.cat((f(x1, x2), g(x1, x2)))\n",
    "y.backward(gradient=torch.tensor([1, 0]), retain_graph=True)\n",
    "print(f\"f({x1[0]}, {x2[0]})={y[0]}, gradient=[{x1.grad[0]}, {x2.grad[0]}]\")\n",
    "x1.grad.zero_(), x2.grad.zero_()\n",
    "y.backward(gradient=torch.tensor([0, 1]))\n",
    "print(f\"g({x1[0]}, {x2[0]})={y[1]}, gradient=[{x1.grad[0]}, {x2.grad[0]}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 资料\n",
    "\n",
    "- [自动求导](https://arxiv.org/pdf/1502.05767.pdf)\n",
    "- [矩阵求导](https://zhuanlan.zhihu.com/p/263777564)\n",
    "- [一个简单的自动求导实现](https://zhuanlan.zhihu.com/p/263777564)\n",
    "- [PyTorch自动求导文档](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
